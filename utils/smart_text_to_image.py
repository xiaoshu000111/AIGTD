import os
import json
import re
from PIL import Image, ImageDraw, ImageFont
import textwrap

'''
æ™ºèƒ½æ¸²æŸ“å‡½æ•°ï¼Œæ”¯æŒä¸­æ–‡æŒ‰å­—æ¢è¡Œï¼Œè‹±æ–‡æŒ‰å•è¯æ¢è¡Œï¼Œé‡æ–°ç”Ÿæˆäº†æ²¡æœ‰å¼•å…¥å™ªå£°çš„é•¿å›¾ã€‚
'''

# === å…¨å±€é…ç½® ===
# å›¾ç‰‡å®½åº¦ (Swin-Base æ¨è 384 æˆ– 448)
IMG_WIDTH = 384 
# å­—ä½“å¤§å° (é€‚ä¸­å³å¯ï¼Œå¤ªå°äº†è™½ç„¶å­—å¤šä½†çœ‹ä¸æ¸…ï¼Œå¤ªå¤§äº†å†…å®¹å°‘)
FONT_SIZE = 18
# è¾¹è·
MARGIN = 20

def get_windows_font(size=20):
    """
    Windows ä¸“ç”¨å­—ä½“åŠ è½½å™¨
    ä¼˜å…ˆåŠ è½½å¾®è½¯é›…é»‘ (msyh.ttc) æˆ– é»‘ä½“ (simhei.ttf)
    """
    # Windows å­—ä½“ç›®å½•
    windows_font_dir = r"D:\Downloads\SiYuanHeiTi-Regular\SiYuanHeiTi-Regular"
    
    # ä¼˜å…ˆçº§åˆ—è¡¨
    font_names = [
        "SourceHanSansSC-Regular-2.otf", # æ€æºé»‘ä½“
        "msyh.ttc",   # å¾®è½¯é›…é»‘ (æœ€æ¸…æ™°ï¼Œé¦–é€‰)
        "simhei.ttf", # é»‘ä½“
        "simsun.ttc", # å®‹ä½“
        "arial.ttf"   # è‹±æ–‡ä¿åº•
    ]
    
    for name in font_names:
        font_path = os.path.join(windows_font_dir, name)
        if os.path.exists(font_path):
            try:
                return ImageFont.truetype(font_path, size)
            except Exception as e:
                continue
    
    print("âš ï¸ æœªæ‰¾åˆ°å¸¸ç”¨ä¸­æ–‡å­—ä½“ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤å­—ä½“...")
    return ImageFont.load_default()

def render_text_smartly(text, save_path):
    """
    ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ™ºèƒ½æ’ç‰ˆæ¸²æŸ“å‡½æ•°
    1. è‹±æ–‡æŒ‰å•è¯æ¢è¡Œï¼Œä¸åˆ‡æ–­ã€‚
    2. ä¸­æ–‡æŒ‰å­—æ¢è¡Œã€‚
    3. ç”Ÿæˆé•¿å›¾ (Long Image)ï¼Œå®½åº¦å›ºå®šä¸º IMG_WIDTHã€‚
    """
    # 1. åŠ è½½å­—ä½“
    font = get_windows_font(FONT_SIZE)
    
    # 2. å‡†å¤‡ç”»å¸ƒå‚æ•°
    max_text_width = IMG_WIDTH - 2 * MARGIN
    lines = []
    current_line = ""
    
    # åˆ›å»ºè™šæ‹Ÿç”»å¸ƒç”¨äºè®¡ç®—æ–‡å­—å®½åº¦
    dummy_draw = ImageDraw.Draw(Image.new('RGB', (1, 1)))
    
    # 3. æ–‡æœ¬æ¸…æ´—ä¸åˆ†æ®µ
    # å¦‚æœæ–‡æœ¬é‡ŒåŸæœ¬å°±æœ‰æ¢è¡Œç¬¦ï¼Œå…ˆæŒ‰æ¢è¡Œç¬¦åˆ‡åˆ†ï¼Œä¿æŒæ®µè½ç»“æ„
    # è¿™ä¸€æ­¥å¯¹äºä¿ç•™ AI çš„â€œåˆ—è¡¨ç‰¹å¾â€å¾ˆé‡è¦
    paragraphs = text.split('\n')
    
    for para in paragraphs:
        # æ­£åˆ™åˆ‡åˆ†ï¼šä¸­æ–‡æŒ‰å­—ï¼Œè‹±æ–‡æŒ‰å•è¯
        # r'[\u4e00-\u9fa5]': åŒ¹é…å•ä¸ªæ±‰å­—
        # r'[^\u4e00-\u9fa5\s]+': åŒ¹é…è¿ç»­çš„éæ±‰å­—éç©ºæ ¼å­—ç¬¦ (å³è‹±æ–‡å•è¯ã€æ ‡ç‚¹)
        # r'\s+': åŒ¹é…ç©ºæ ¼
        atoms = re.findall(r'[\u4e00-\u9fa5]|[^\u4e00-\u9fa5\s]+|\s+', para)
        
        for atom in atoms:
            test_line = current_line + atom
            
            # è®¡ç®—å®½åº¦ (å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ Pillow)
            if hasattr(font, 'getlength'):
                width = font.getlength(test_line)
            else:
                width = dummy_draw.textlength(test_line, font=font)
                
            if width <= max_text_width:
                # å¦‚æœèƒ½æ”¾ä¸‹ï¼Œå°±åŠ åˆ°å½“å‰è¡Œ
                current_line = test_line
            else:
                # æ”¾ä¸ä¸‹ï¼Œå…ˆæŠŠå½“å‰è¡Œå­˜å…¥ lines
                if current_line:
                    lines.append(current_line)
                
                # å¤„ç†å•ä¸ªåŸå­è¿‡é•¿çš„æƒ…å†µ (æå°‘è§ï¼Œæ¯”å¦‚è¶…é•¿ç½‘å€)
                atom_width = font.getlength(atom) if hasattr(font, 'getlength') else dummy_draw.textlength(atom, font=font)
                if atom_width > max_text_width:
                    current_line = atom # æ²¡åŠæ³•åªèƒ½å¼ºåˆ¶æ¢è¡Œ
                else:
                    current_line = atom.lstrip() # æ–°èµ·ä¸€è¡Œï¼Œå»æ‰å¼€å¤´çš„ç©ºæ ¼
        
        # æ®µè½ç»“æŸï¼ŒæŠŠç¼“å†²åŒºå†…å®¹åŠ å…¥
        if current_line:
            lines.append(current_line)
            current_line = ""
            
        # (å¯é€‰) å¦‚æœå¸Œæœ›æ®µè½é—´æœ‰é¢å¤–ç©ºè¡Œï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢æ³¨é‡Š
        # lines.append("") 

    # 4. è®¡ç®—å›¾ç‰‡é«˜åº¦
    # è¡Œé«˜è®¾ç½®ä¸ºå­—ä½“çš„ 1.5 å€ï¼Œæä¾›è‰¯å¥½çš„é˜…è¯»å‘¼å¸æ„Ÿ
    line_height = int(FONT_SIZE * 1.5)
    # æ€»é«˜åº¦ = è¡Œæ•° * è¡Œé«˜ + ä¸Šä¸‹è¾¹è·
    image_height = len(lines) * line_height + 2 * MARGIN
    
    # ä¿è¯å›¾ç‰‡è‡³å°‘æ˜¯æ­£æ–¹å½¢ (é˜²æ­¢æ–‡å­—å¤ªå°‘å¯¼è‡´å›¾ç‰‡å¤ªæ‰ï¼ŒResNet/Swin å–œæ¬¢æ­£æ–¹å½¢)
    if image_height < IMG_WIDTH:
        image_height = IMG_WIDTH

    # 5. ç»˜åˆ¶å›¾ç‰‡
    img = Image.new('RGB', (IMG_WIDTH, image_height), (255, 255, 255)) # ç™½åº•
    draw = ImageDraw.Draw(img)
    
    y_text = MARGIN
    for line in lines:
        draw.text((MARGIN, y_text), line, font=font, fill=(0, 0, 0)) # é»‘å­—
        y_text += line_height
        
    img.save(save_path)

# ==========================================
# ä¸‹é¢æ˜¯ä½ çš„æ•°æ®å¤„ç†é€»è¾‘ (å·²æ›´æ–°è°ƒç”¨)
# ==========================================

def process_clean_hc3_qa(train_jsonl, test_jsonl, output_base_dir):
    """å¤„ç† clean_hc3_qa æ•°æ®"""
    
    # åˆ›å»ºç›®å½•ç»“æ„
    train_human_dir = os.path.join(output_base_dir, "train_data", "human")
    train_ai_dir = os.path.join(output_base_dir, "train_data", "ai")
    test_human_dir = os.path.join(output_base_dir, "test_data", "human")
    test_ai_dir = os.path.join(output_base_dir, "test_data", "ai")
    
    for folder in [train_human_dir, train_ai_dir, test_human_dir, test_ai_dir]:
        os.makedirs(folder, exist_ok=True)
    
    print("æ­£åœ¨å¤„ç†è®­ç»ƒé›†...")
    _process_dataset(train_jsonl, train_human_dir, train_ai_dir, "train")
    
    print("æ­£åœ¨å¤„ç†æµ‹è¯•é›†...")
    _process_dataset(test_jsonl, test_human_dir, test_ai_dir, "test")
    
    print("\nâœ“ æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼")

def _process_dataset(jsonl_path, human_folder, ai_folder, dataset_type):
    if not os.path.exists(jsonl_path):
        print(f"âœ— æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}")
        return
    
    count = 0
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, start=1):
            try:
                data = json.loads(line.strip())
                label = data.get('label', '').strip().lower()
                text = data.get('article', '').strip()
                
                # ã€é‡è¦ã€‘è¿™é‡Œæˆ‘å»æ‰äº† replace('\\n', '')
                # å› ä¸ºæ–°çš„æ¸²æŸ“å‡½æ•° render_text_smartly å¯ä»¥å¾ˆå¥½åœ°å¤„ç†æ®µè½ç»“æ„
                # å¦‚æœä½ æƒ³æ¢å¤æˆä¸€è¡Œï¼Œå¯ä»¥å–æ¶ˆä¸‹é¢æ³¨é‡Š
                # text = text.replace('\\n', ' ').replace('\n', ' ')
                
                if not text: continue
                
                if label == 'human':
                    output_path = os.path.join(human_folder, f"{line_num}.png")
                    render_text_smartly(text, output_path) # <--- æ›¿æ¢ä¸ºæ–°å‡½æ•°
                elif label == 'machine':
                    output_path = os.path.join(ai_folder, f"{line_num}.png")
                    render_text_smartly(text, output_path) # <--- æ›¿æ¢ä¸ºæ–°å‡½æ•°
                
                count += 1
                if count % 100 == 0:
                    print(f"Processing {dataset_type}: {count}...", end='\r')
                    
            except Exception as e:
                print(f"Error line {line_num}: {e}")

def process_nlpcc_train_data(json_path, output_base_dir):
    """å¤„ç† NLPCC è®­ç»ƒæ•°æ®"""
    
    human_dir = os.path.join(output_base_dir, "human")
    ai_dir = os.path.join(output_base_dir, "ai")
    
    os.makedirs(human_dir, exist_ok=True)
    os.makedirs(ai_dir, exist_ok=True)
    
    if not os.path.exists(json_path):
        print(f"âœ— æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
        return
    
    print(f"ğŸš€ å¼€å§‹å¤„ç†: {json_path}")
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data_list = json.load(f)
    
    count = 0
    for item_id, item in enumerate(data_list, start=1):
        try:
            text = item.get('text', '').strip()
            label = item.get('label', -1)
            item_id_val = item.get('id', item_id)
            
            # åŒæ ·ä¿ç•™æ¢è¡Œç¬¦ï¼Œäº¤ç»™æ¸²æŸ“å‡½æ•°å¤„ç†
            if not text: continue
            
            if label == 0: # human
                output_path = os.path.join(human_dir, f"{item_id_val}.png")
                render_text_smartly(text, output_path) # <--- æ›¿æ¢ä¸ºæ–°å‡½æ•°
                count += 1
            elif label == 1: # ai
                output_path = os.path.join(ai_dir, f"{item_id_val}.png")
                render_text_smartly(text, output_path) # <--- æ›¿æ¢ä¸ºæ–°å‡½æ•°
                count += 1
                
            if count % 100 == 0:
                print(f"å·²ç”Ÿæˆ {count} å¼ ...", end='\r')
                
        except Exception as e:
            print(f"Error ID {item_id_val}: {e}")

    print(f"\nğŸ‰ å¤„ç†å®Œæˆ! å…± {count} å¼ ã€‚")

    

# === ä¸»ç¨‹åºå…¥å£ ===
if __name__ == "__main__":
    
    # è¯·æ ¹æ®ä½ çš„å®é™…æƒ…å†µå–æ¶ˆæ³¨é‡Šè¿è¡Œ
    
    # ä»»åŠ¡ 1: å¤„ç† NLPCC æ•°æ®
    # ------------------------------------------------
    print(">>> ä»»åŠ¡ 1: å¤„ç† NLPCC æ•°æ®")
    json_file = r"d:\Desktop\æ–‡æœ¬æ£€æµ‹\NLPCC-2025-Task1-main\data\test_with_label.json"
    output_dir = r"/dataset/detectRL-zh/384/test"
    process_nlpcc_train_data(json_file, output_dir)
    
    
    # ä»»åŠ¡ 2: å¤„ç† HC3 æ•°æ® (å¦‚æœæœ‰éœ€è¦)
    # ------------------------------------------------
    # print("\n>>> ä»»åŠ¡ 2: å¤„ç† clean_hc3_qa æ•°æ®")
    # train_jsonl = r"d:\Desktop\æ–‡æœ¬æ£€æµ‹\HC3-Chinese\clean_hc3_qa\train.jsonl"
    # test_jsonl = r"d:\Desktop\æ–‡æœ¬æ£€æµ‹\HC3-Chinese\clean_hc3_qa\test.jsonl"
    # hc3_output_dir = r"d:\Desktop\æ–‡æœ¬æ£€æµ‹\HC3-Chinese\dataset\clean"
    # process_clean_hc3_qa(train_jsonl, test_jsonl, hc3_output_dir)
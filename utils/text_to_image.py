from PIL import Image, ImageDraw, ImageFont
import textwrap
import os
import json

def text_to_long_image(text, output_path, img_width=224):
    """
    å°†æ–‡æœ¬è½¬æ¢ä¸ºé•¿å›¾ç‰‡
    img_width: å›ºå®šå®½åº¦ï¼Œå»ºè®®è®¾ä¸º 300 æˆ– 400ï¼Œæ—¢èƒ½ä½“ç°æ®µè½ç»“æ„ï¼Œåˆæ–¹ä¾¿åç»­è£å‰ª
    """
    # 1. è®¾ç½®å­—ä½“ (Windows)
    try:
        font_size = 14
        # ä¼˜å…ˆä½¿ç”¨å¾®è½¯é›…é»‘ (msyh.ttf) æˆ– é»‘ä½“ (simhei.ttf)
        font = ImageFont.truetype("C:/Windows/Fonts/simhei.ttf", font_size)
    except:
        font = ImageFont.load_default()
        print("æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“ï¼ˆå¯èƒ½ä¹±ç ï¼‰")

    # 2. æ–‡æœ¬æ¢è¡Œè®¡ç®—
    chars_per_line = int(img_width / font_size) - 2 
    lines = textwrap.wrap(text, width=chars_per_line)
    
    # 3. åŠ¨æ€è®¡ç®—é«˜åº¦
    line_spacing = 6  # è¡Œé—´è·
    line_height = font_size + line_spacing
    
    # è®¡ç®—éœ€è¦çš„æ€»é«˜åº¦
    content_height = len(lines) * line_height + 40 # 40æ˜¯ä¸Šä¸‹è¾¹è·
    
    # å¼ºåˆ¶æœ€å°é«˜åº¦ä¸º 224 (ä¸ºäº†åç»­æ–¹ä¾¿å–‚ç»™ ResNet)
    final_height = max(224, content_height)
    
    # 4. åˆ›å»ºç”»å¸ƒ
    img = Image.new('RGB', (img_width, final_height), color='white')
    d = ImageDraw.Draw(img)
    
    # 5. ç»˜åˆ¶æ–‡æœ¬
    y_text = 20
    for line in lines:
        d.text((15, y_text), line, font=font, fill=(0, 0, 0)) # 15æ˜¯å·¦è¾¹è·
        y_text += line_height
        
    img.save(output_path)

def text_to_fixed_image_sure(text, output_path, img_size=(224, 224), font_size=14, left_margin=15, right_margin=15):
    """
    å°†æ–‡æœ¬è½¬æ¢ä¸ºå›ºå®šå¤§å°çš„å›¾ç‰‡ (224, 224)
    
    å‚æ•°ï¼š
    - text: è¦è½¬æ¢çš„æ–‡æœ¬
    - output_path: è¾“å‡ºå›¾ç‰‡è·¯å¾„
    - img_size: ç”»å¸ƒå¤§å°ï¼Œå›ºå®šä¸º (224, 224)
    - font_size: å­—ä½“å¤§å°ï¼Œå›ºå®šä¸º 14px
    - left_margin: å·¦è¾¹è·ï¼Œé»˜è®¤ 15px
    - right_margin: å³è¾¹è·ï¼Œé»˜è®¤ 15px
    """
    # 1. è®¾ç½®å­—ä½“ (Windows)
    try:
        font = ImageFont.truetype("C:/Windows/Fonts/simhei.ttf", font_size)
    except:
        font = ImageFont.load_default()
        print("æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“ï¼ˆå¯èƒ½ä¹±ç ï¼‰")
    
    # 2. è®¡ç®—æ’ç‰ˆå‚æ•°
    img_width, img_height = img_size
    available_width = img_width - left_margin - right_margin  # å¯ç”¨å®½åº¦ï¼š224-15-15=194
    chars_per_line = max(1, int(available_width / font_size))  # æ¯è¡Œå­—ç¬¦æ•°ï¼šçº¦194/14=13-14ä¸ªå­—
    
    line_spacing = 4  # è¡Œé—´è·
    line_height = font_size + line_spacing  # æ¯è¡Œé«˜åº¦ï¼š14+4=18px
    
    max_lines = int((img_height - 10) / line_height)  # æœ€å¤šè¡Œæ•°ï¼š(224-10)/18â‰ˆ11-12è¡Œ
    max_total_chars = chars_per_line * max_lines  # æœ€å¤šå­—ç¬¦æ•°ï¼š13*11â‰ˆ143ä¸ªå­—
    
    # 3. æˆªæ–­é•¿æ–‡æœ¬
    if len(text) > max_total_chars:
        text = text[:max_total_chars]
    
    # 4. æ–‡æœ¬æ¢è¡Œå¤„ç†
    lines = textwrap.wrap(text, width=chars_per_line)
    lines = lines[:max_lines]  # åªä¿ç•™èƒ½æ˜¾ç¤ºçš„è¡Œæ•°
    
    # 5. åˆ›å»ºç”»å¸ƒ
    img = Image.new('RGB', img_size, color='white')
    draw = ImageDraw.Draw(img)
    
    # 6. ç»˜åˆ¶æ–‡æœ¬
    y_offset = 5  # ä¸Šè¾¹è·
    for line in lines:
        draw.text((left_margin, y_offset), line, font=font, fill=(0, 0, 0))
        y_offset += line_height
        
        # é˜²æ­¢è¶…å‡ºç”»å¸ƒ
        if y_offset >= img_height:
            break
    
    img.save(output_path)


def process_all_jsonl_sure(jsonl_path, human_folder, ai_folder, img_width=224):
    """
    å¤„ç†jsonlæ–‡ä»¶ï¼Œå°†human_answerså’Œchatgpt_answersè½¬æ¢ä¸ºå›ºå®šå¤§å°(224x224)çš„å›¾ç‰‡
    
    å‚æ•°ï¼š
    - jsonl_path: jsonlæ–‡ä»¶è·¯å¾„
    - human_folder: humanç­”æ¡ˆè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    - ai_folder: chatgptç­”æ¡ˆè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    - img_width: å›¾ç‰‡å®½åº¦ï¼Œé»˜è®¤224
    """
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(human_folder, exist_ok=True)
    os.makedirs(ai_folder, exist_ok=True)
    
    # è¯»å–å’Œå¤„ç†jsonlæ–‡ä»¶
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, start=0):  # ä»0å¼€å§‹ç¼–å·
            try:
                data = json.loads(line.strip())
                
                # æå–human_answerså’Œchatgpt_answers
                human_answer = data.get('human_answers', [''])[0]  # å–ç¬¬ä¸€æ¡ç­”æ¡ˆ
                chatgpt_answer = data.get('chatgpt_answers', [''])[0]

                # å°†æ‰€æœ‰æ¢è¡Œç¬¦æ›¿æ¢æˆç©ºå­—ç¬¦ä¸²ï¼Œä½¿æ–‡æœ¬æˆä¸ºå•è¡Œ
                human_answer = human_answer.replace('\\n', '').replace('\n', '').replace('\r', '')
                chatgpt_answer = chatgpt_answer.replace('\\n', '').replace('\n', '').replace('\r', '')
                
                # ç”Ÿæˆå›¾ç‰‡
                if human_answer:
                    human_img_path = os.path.join(human_folder, f"{line_num+1}.png")
                    text_to_fixed_image_sure(human_answer, human_img_path)
                    print(f"âœ“ å·²ç”Ÿæˆ human å›¾ç‰‡: {line_num+1}.png")
                
                if chatgpt_answer:
                    ai_img_path = os.path.join(ai_folder, f"{line_num+1}.png")
                    text_to_fixed_image_sure(chatgpt_answer, ai_img_path)
                    print(f"âœ“ å·²ç”Ÿæˆ ai å›¾ç‰‡: {line_num+1}.png")
                    
            except json.JSONDecodeError as e:
                print(f"âœ— ç¬¬ {line_num+1} è¡ŒJSONè§£æå¤±è´¥: {e}")
            except Exception as e:
                print(f"âœ— ç¬¬ {line_num+1} è¡Œå¤„ç†å¤±è´¥: {e}")



def process_clean_hc3_qa(train_jsonl, test_jsonl, output_base_dir):
    """
    å¤„ç† clean_hc3_qa æ•°æ®ï¼Œæ ¹æ® label å­—æ®µå’Œæ•°æ®é›†ç±»å‹å°†æ•°æ®è½¬æ¢ä¸ºå›¾ç‰‡
    
    å‚æ•°ï¼š
    - train_jsonl: è®­ç»ƒé›†JSONLæ–‡ä»¶è·¯å¾„
    - test_jsonl: æµ‹è¯•é›†JSONLæ–‡ä»¶è·¯å¾„
    - output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•ï¼ˆcleanæ–‡ä»¶å¤¹è·¯å¾„ï¼‰
    """
    
    # åˆ›å»ºç›®å½•ç»“æ„
    train_human_dir = os.path.join(output_base_dir, "train_data", "human")
    train_ai_dir = os.path.join(output_base_dir, "train_data", "ai")
    test_human_dir = os.path.join(output_base_dir, "test_data", "human")
    test_ai_dir = os.path.join(output_base_dir, "test_data", "ai")
    
    # åˆ›å»ºæ‰€æœ‰è¾“å‡ºæ–‡ä»¶å¤¹
    for folder in [train_human_dir, train_ai_dir, test_human_dir, test_ai_dir]:
        os.makedirs(folder, exist_ok=True)
    
    # å¤„ç†è®­ç»ƒé›†
    print("=" * 50)
    print("å¤„ç†è®­ç»ƒé›†æ•°æ®...")
    print("=" * 50)
    _process_dataset(train_jsonl, train_human_dir, train_ai_dir, "train")
    
    # å¤„ç†æµ‹è¯•é›†
    print("\n" + "=" * 50)
    print("å¤„ç†æµ‹è¯•é›†æ•°æ®...")
    print("=" * 50)
    _process_dataset(test_jsonl, test_human_dir, test_ai_dir, "test")
    
    print("\nâœ“ æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼")


def _process_dataset(jsonl_path, human_folder, ai_folder, dataset_type):
    """
    å¤„ç†å•ä¸ªæ•°æ®é›†
    
    å‚æ•°ï¼š
    - jsonl_path: JSONLæ–‡ä»¶è·¯å¾„
    - human_folder: humanæ ‡ç­¾è¾“å‡ºæ–‡ä»¶å¤¹
    - ai_folder: aiæ ‡ç­¾è¾“å‡ºæ–‡ä»¶å¤¹
    - dataset_type: æ•°æ®é›†ç±»å‹ï¼ˆtrain æˆ– testï¼‰
    """
    
    if not os.path.exists(jsonl_path):
        print(f"âœ— æ–‡ä»¶ä¸å­˜åœ¨: {jsonl_path}")
        return
    
    train_count = 0
    ai_count = 0
    error_count = 0
    
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, start=1):
            try:
                data = json.loads(line.strip())
                
                # è·å–å…³é”®å­—æ®µ
                label = data.get('label', '').strip().lower()
                text = data.get('article', '').strip()
                
                # æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                text = text.replace('\\n', '').replace('\n', '').replace('\r', '')
                
                if not text:
                    print(f"âš  ç¬¬ {line_num} è¡Œï¼šæ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # æ ¹æ®labelåˆ¤æ–­è¾“å‡ºç›®å½•
                if label == 'human':
                    output_path = os.path.join(human_folder, f"{line_num}.png")
                    text_to_fixed_image_sure(text, output_path)
                    print(f"âœ“ [{dataset_type}] å·²ç”Ÿæˆ human å›¾ç‰‡: {line_num}.png")
                    train_count += 1
                    
                elif label == 'machine':
                    output_path = os.path.join(ai_folder, f"{line_num}.png")
                    text_to_fixed_image_sure(text, output_path)
                    print(f"âœ“ [{dataset_type}] å·²ç”Ÿæˆ ai å›¾ç‰‡: {line_num}.png")
                    ai_count += 1
                    
                else:
                    print(f"âš  ç¬¬ {line_num} è¡Œï¼šæ ‡ç­¾å€¼ '{label}' ä¸è¯†åˆ«ï¼Œè·³è¿‡")
                    
            except json.JSONDecodeError as e:
                print(f"âœ— ç¬¬ {line_num} è¡ŒJSONè§£æå¤±è´¥: {e}")
                error_count += 1
            except Exception as e:
                print(f"âœ— ç¬¬ {line_num} è¡Œå¤„ç†å¤±è´¥: {e}")
                error_count += 1
    
    print(f"\nğŸ“Š [{dataset_type}] ç»Ÿè®¡ç»“æœ:")
    print(f"   - human å›¾ç‰‡: {train_count} å¼ ")
    print(f"   - ai å›¾ç‰‡: {ai_count} å¼ ")
    print(f"   - é”™è¯¯: {error_count} æ¡")
    print(f"   - æ€»è®¡: {train_count + ai_count} å¼ ")


def process_all_jsonl_sure(jsonl_path, human_folder, ai_folder, img_width=224):
    """
    å¤„ç†jsonlæ–‡ä»¶ï¼Œå°†human_answerså’Œchatgpt_answersè½¬æ¢ä¸ºå›ºå®šå¤§å°(224x224)çš„å›¾ç‰‡
    
    å‚æ•°ï¼š
    - jsonl_path: jsonlæ–‡ä»¶è·¯å¾„
    - human_folder: humanç­”æ¡ˆè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    - ai_folder: chatgptç­”æ¡ˆè¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    - img_width: å›¾ç‰‡å®½åº¦ï¼Œé»˜è®¤224
    """
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(human_folder, exist_ok=True)
    os.makedirs(ai_folder, exist_ok=True)
    
    # è¯»å–å’Œå¤„ç†jsonlæ–‡ä»¶
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, start=0):  # ä»0å¼€å§‹ç¼–å·
            try:
                data = json.loads(line.strip())
                
                # æå–human_answerså’Œchatgpt_answers
                human_answer = data.get('human_answers', [''])[0]  # å–ç¬¬ä¸€æ¡ç­”æ¡ˆ
                chatgpt_answer = data.get('chatgpt_answers', [''])[0]

                # å°†æ‰€æœ‰æ¢è¡Œç¬¦æ›¿æ¢æˆç©ºå­—ç¬¦ä¸²ï¼Œä½¿æ–‡æœ¬æˆä¸ºå•è¡Œ
                human_answer = human_answer.replace('\\n', '').replace('\n', '').replace('\r', '')
                chatgpt_answer = chatgpt_answer.replace('\\n', '').replace('\n', '').replace('\r', '')
                
                # ç”Ÿæˆå›¾ç‰‡
                if human_answer:
                    human_img_path = os.path.join(human_folder, f"{line_num+1}.png")
                    text_to_fixed_image_sure(human_answer, human_img_path)
                    print(f"âœ“ å·²ç”Ÿæˆ human å›¾ç‰‡: {line_num+1}.png")
                
                if chatgpt_answer:
                    ai_img_path = os.path.join(ai_folder, f"{line_num+1}.png")
                    text_to_fixed_image_sure(chatgpt_answer, ai_img_path)
                    print(f"âœ“ å·²ç”Ÿæˆ ai å›¾ç‰‡: {line_num+1}.png")
                    
            except json.JSONDecodeError as e:
                print(f"âœ— ç¬¬ {line_num+1} è¡ŒJSONè§£æå¤±è´¥: {e}")
            except Exception as e:
                print(f"âœ— ç¬¬ {line_num+1} è¡Œå¤„ç†å¤±è´¥: {e}")


def process_nlpcc_test_data(json_path, output_base_dir):
    """
    å¤„ç† NLPCC æµ‹è¯•æ•°æ®ï¼Œæ ¹æ® label å­—æ®µå°†æ•°æ®è½¬æ¢ä¸ºå›¾ç‰‡
    
    å‚æ•°ï¼š
    - json_path: JSON æ–‡ä»¶è·¯å¾„ (train.json æˆ– test.json)
    - output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•è·¯å¾„ (dataset/detectRL-zh/test)
    
    label = 0 -> human æ–‡ä»¶å¤¹
    label = 1 -> ai æ–‡ä»¶å¤¹
    """
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    human_dir = os.path.join(output_base_dir, "human")
    ai_dir = os.path.join(output_base_dir, "ai")
    
    os.makedirs(human_dir, exist_ok=True)
    os.makedirs(ai_dir, exist_ok=True)
    
    if not os.path.exists(json_path):
        print(f"âœ— æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
        return
    
    human_count = 0
    ai_count = 0
    error_count = 0
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data_list = json.load(f)
        
        for item_id, item in enumerate(data_list, start=1):
            try:
                # è·å–å…³é”®å­—æ®µ
                text = item.get('text', '').strip()
                label = item.get('label', -1)
                item_id_val = item.get('id', item_id)
                
                # æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                text = text.replace('\\n', '').replace('\n', '').replace('\r', '')
                
                if not text:
                    print(f"âš  ID {item_id_val}ï¼šæ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # æ ¹æ® label åˆ¤æ–­è¾“å‡ºç›®å½•
                if label == 0:
                    # human æ ‡ç­¾
                    output_path = os.path.join(human_dir, f"{item_id_val}.png")
                    text_to_fixed_image_sure(text, output_path)
                    print(f"âœ“ å·²ç”Ÿæˆ human å›¾ç‰‡: {item_id_val}.png")
                    human_count += 1
                    
                elif label == 1:
                    # ai æ ‡ç­¾
                    output_path = os.path.join(ai_dir, f"{item_id_val}.png")
                    text_to_fixed_image_sure(text, output_path)
                    print(f"âœ“ å·²ç”Ÿæˆ ai å›¾ç‰‡: {item_id_val}.png")
                    ai_count += 1
                    
                else:
                    print(f"âš  ID {item_id_val}ï¼šæ ‡ç­¾å€¼ '{label}' ä¸è¯†åˆ«ï¼Œè·³è¿‡")
                    
            except Exception as e:
                print(f"âœ— ID {item_id_val} å¤„ç†å¤±è´¥: {e}")
                error_count += 1
        
        # æ‰“å°ç»Ÿè®¡ç»“æœ
        print(f"\n{'='*50}")
        print(f"ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:")
        print(f"   - human å›¾ç‰‡: {human_count} å¼ ")
        print(f"   - ai å›¾ç‰‡: {ai_count} å¼ ")
        print(f"   - é”™è¯¯: {error_count} æ¡")
        print(f"   - æ€»è®¡: {human_count + ai_count} å¼ ")
        print(f"{'='*50}\n")
        
    except json.JSONDecodeError as e:
        print(f"âœ— JSON è§£æå¤±è´¥: {e}")
    except Exception as e:
        print(f"âœ— å¤„ç†è¿‡ç¨‹å‡ºé”™: {e}")

def process_nlpcc_train_data(json_path, output_base_dir):
    """
    å¤„ç† NLPCC æµ‹è¯•æ•°æ®ï¼Œæ ¹æ® label å­—æ®µå°†æ•°æ®è½¬æ¢ä¸ºå›¾ç‰‡
    
    å‚æ•°ï¼š
    - json_path: JSON æ–‡ä»¶è·¯å¾„ (train.json æˆ– test.json)
    - output_base_dir: è¾“å‡ºåŸºç¡€ç›®å½•è·¯å¾„ (dataset/detectRL-zh/test)
    
    label = 0 -> human æ–‡ä»¶å¤¹
    label = 1 -> ai æ–‡ä»¶å¤¹
    """
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    human_dir = os.path.join(output_base_dir, "human")
    ai_dir = os.path.join(output_base_dir, "ai")
    
    os.makedirs(human_dir, exist_ok=True)
    os.makedirs(ai_dir, exist_ok=True)
    
    if not os.path.exists(json_path):
        print(f"âœ— æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")
        return
    
    human_count = 0
    ai_count = 0
    error_count = 0
    
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data_list = json.load(f)
        
        for item_id, item in enumerate(data_list, start=1):
            try:
                # è·å–å…³é”®å­—æ®µ
                text = item.get('text', '').strip()
                label = item.get('label', -1)
                item_id_val = item.get('id', item_id)
                
                # æ¸…ç†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                text = text.replace('\\n', '').replace('\n', '').replace('\r', '')
                
                if not text:
                    print(f"âš  ID {item_id_val}ï¼šæ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # æ ¹æ® label åˆ¤æ–­è¾“å‡ºç›®å½•
                if label == 0:
                    # human æ ‡ç­¾
                    output_path = os.path.join(human_dir, f"{item_id_val}.png")
                    text_to_fixed_image_sure(text, output_path)
                    print(f"âœ“ å·²ç”Ÿæˆ human å›¾ç‰‡: {item_id_val}.png")
                    human_count += 1
                    
                elif label == 1:
                    # ai æ ‡ç­¾
                    output_path = os.path.join(ai_dir, f"{item_id_val}.png")
                    text_to_fixed_image_sure(text, output_path)
                    print(f"âœ“ å·²ç”Ÿæˆ ai å›¾ç‰‡: {item_id_val}.png")
                    ai_count += 1
                    
                else:
                    print(f"âš  ID {item_id_val}ï¼šæ ‡ç­¾å€¼ '{label}' ä¸è¯†åˆ«ï¼Œè·³è¿‡")
                    
            except Exception as e:
                print(f"âœ— ID {item_id_val} å¤„ç†å¤±è´¥: {e}")
                error_count += 1
        
        # æ‰“å°ç»Ÿè®¡ç»“æœ
        print(f"\n{'='*50}")
        print(f"ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:")
        print(f"   - human å›¾ç‰‡: {human_count} å¼ ")
        print(f"   - ai å›¾ç‰‡: {ai_count} å¼ ")
        print(f"   - é”™è¯¯: {error_count} æ¡")
        print(f"   - æ€»è®¡: {human_count + ai_count} å¼ ")
        print(f"{'='*50}\n")
        
    except json.JSONDecodeError as e:
        print(f"âœ— JSON è§£æå¤±è´¥: {e}")
    except Exception as e:
        print(f"âœ— å¤„ç†è¿‡ç¨‹å‡ºé”™: {e}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å¤„ç† NLPCC æµ‹è¯•æ•°æ®
    print("å¼€å§‹å¤„ç† NLPCC æµ‹è¯•æ•°æ®...\n")
    
    json_file = r"d:\Desktop\æ–‡æœ¬æ£€æµ‹\NLPCC-2025-Task1-main\data\train.json"
    output_dir = r"d:\Desktop\æ–‡æœ¬æ£€æµ‹\HC3-Chinese\dataset\detectRL-zh\train"
    
    process_nlpcc_train_data(json_file, output_dir)

'''
# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ–¹å¼1: å¤„ç† clean_hc3_qa æ•°æ®
    print("å¼€å§‹å¤„ç† clean_hc3_qa æ•°æ®...\n")
    
    train_jsonl = r"d:\Desktop\æ–‡æœ¬æ£€æµ‹\HC3-Chinese\clean_hc3_qa\train.jsonl"
    test_jsonl = r"d:\Desktop\æ–‡æœ¬æ£€æµ‹\HC3-Chinese\clean_hc3_qa\test.jsonl"
    output_dir = r"d:\Desktop\æ–‡æœ¬æ£€æµ‹\HC3-Chinese\dataset\clean"
    
    process_clean_hc3_qa(train_jsonl, test_jsonl, output_dir)

'''